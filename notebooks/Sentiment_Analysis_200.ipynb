{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KN_sBufai4c_",
        "outputId": "5892e8f3-4bcb-4a43-aa91-aebef9a30aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gensim\n",
        "# Made with help from GPT\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from typing import List, Optional, Any\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "# nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ocwqm-ayjDFL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and shuffle documents\n",
        "documents : list[tuple[list[str], str]]  = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Extract only the review texts for training Word2Vec\n",
        "sentences = [list(map(str.lower, movie_reviews.words(fileid)))\n",
        "             for fileid in movie_reviews.fileids()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GnXjFN_jFKq"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model (Skip-Gram)\n",
        "embedding_dim = 200\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=embedding_dim,\n",
        "    window=15,\n",
        "    min_count=2,\n",
        "    sg=1,  # Skip-Gram\n",
        "    workers=8,\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lfjWwzD5jKkb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_vector(word: str) -> Optional[np.ndarray]:\n",
        "    \"\"\"Fetch vector from Word2Vec model if word exists.\"\"\"\n",
        "    return w2v_model.wv[word] if word in w2v_model.wv else None\n",
        "\n",
        "def get_review_vector_avg(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute average vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "def get_review_vector_max(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute max-pooled vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.max(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "X_avg = []\n",
        "X_max = []\n",
        "y = []\n",
        "\n",
        "for words, label in documents:\n",
        "    X_avg.append(get_review_vector_avg(words))\n",
        "    X_max.append(get_review_vector_max(words))\n",
        "    y.append(1 if label == 'pos' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MxHyPMCjLNu",
        "outputId": "eac6f87f-2982-4aa8-d40f-ce7cb5d55e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.86      0.83       199\n",
            "           1       0.85      0.78      0.82       201\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.73      0.72       199\n",
            "           1       0.72      0.70      0.71       201\n",
            "\n",
            "    accuracy                           0.71       400\n",
            "   macro avg       0.71      0.71      0.71       400\n",
            "weighted avg       0.71      0.71      0.71       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg = LogisticRegression(max_iter=1000)\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max = LogisticRegression(max_iter=1000)\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMwc96EF3n-3",
        "outputId": "25e67b49-dded-4b69-ccee-71133b823c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.86      0.89       199\n",
            "           1       0.87      0.93      0.90       201\n",
            "\n",
            "    accuracy                           0.89       400\n",
            "   macro avg       0.89      0.89      0.89       400\n",
            "weighted avg       0.89      0.89      0.89       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.71      0.69       199\n",
            "           1       0.70      0.67      0.69       201\n",
            "\n",
            "    accuracy                           0.69       400\n",
            "   macro avg       0.69      0.69      0.69       400\n",
            "weighted avg       0.69      0.69      0.69       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "scale_data = True\n",
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "if scale_data:\n",
        "    clf_avg = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "    clf_max = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "else:\n",
        "    clf_avg = LogisticRegression(max_iter=1000)\n",
        "    clf_max = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L9VjDTCljNwf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def build_dense_model(input_dim: int) -> tf.keras.Model:\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),             # Cleanly define input shape\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')         # Binary classification\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERVuUaLK0Alj",
        "outputId": "c13dfc78-8313-4e3c-bde2-03ee50feb795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train labels: (array([0, 1]), array([801, 799]))\n",
            "Test labels: (array([0, 1]), array([199, 201]))\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "X_avg_train_np = np.array(X_avg_train)\n",
        "X_avg_test_np = np.array(X_avg_test)\n",
        "X_max_train_np = np.array(X_max_train)\n",
        "X_max_test_np = np.array(X_max_test)\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(\"Train labels:\", np.unique(y_train_np, return_counts=True))\n",
        "print(\"Test labels:\", np.unique(y_test_np, return_counts=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRHgdB9jTIF",
        "outputId": "b36836b2-9218-4a3a-a91f-3f6851d1f292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Scaled!\n",
            "Training model on average vectors...\n",
            "Epoch 1/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5555 - loss: 0.7873 - val_accuracy: 0.7688 - val_loss: 0.5755\n",
            "Epoch 2/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6946 - loss: 0.5727 - val_accuracy: 0.8250 - val_loss: 0.4466\n",
            "Epoch 3/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7868 - loss: 0.4592 - val_accuracy: 0.8562 - val_loss: 0.3221\n",
            "Epoch 4/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8447 - loss: 0.3370 - val_accuracy: 0.8875 - val_loss: 0.2571\n",
            "Epoch 5/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8737 - loss: 0.2874 - val_accuracy: 0.8938 - val_loss: 0.2436\n",
            "Epoch 6/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9011 - loss: 0.2288 - val_accuracy: 0.9125 - val_loss: 0.2498\n",
            "Epoch 7/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9175 - loss: 0.1927 - val_accuracy: 0.9062 - val_loss: 0.2222\n",
            "Epoch 8/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9277 - loss: 0.1654 - val_accuracy: 0.9062 - val_loss: 0.2181\n",
            "Epoch 9/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9479 - loss: 0.1556 - val_accuracy: 0.9000 - val_loss: 0.2357\n",
            "Epoch 10/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.1382 - val_accuracy: 0.9062 - val_loss: 0.2262\n",
            "Epoch 11/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9485 - loss: 0.1325 - val_accuracy: 0.9125 - val_loss: 0.2144\n",
            "Epoch 12/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.1023 - val_accuracy: 0.9125 - val_loss: 0.2537\n",
            "Epoch 13/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.0845 - val_accuracy: 0.8938 - val_loss: 0.3007\n",
            "Epoch 14/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9635 - loss: 0.0891 - val_accuracy: 0.9187 - val_loss: 0.2640\n",
            "Epoch 15/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.0640 - val_accuracy: 0.9062 - val_loss: 0.2971\n",
            "Epoch 16/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0535 - val_accuracy: 0.9187 - val_loss: 0.2572\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Evaluating model on average vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8960 - loss: 0.3239  \n",
            "Test Accuracy (Average Vector): 0.8925\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_avg_train_np_scaled = scaler.fit_transform(X_avg_train_np)\n",
        "  X_avg_test_np_scaled = scaler.transform(X_avg_test_np)\n",
        "  print(\"Data Scaled!\")\n",
        "else:\n",
        "  X_avg_train_np_scaled = X_avg_train_np\n",
        "  X_avg_test_np_scaled = X_avg_test_np\n",
        "\n",
        "\n",
        "# Train on average vectors\n",
        "print(\"Training model on average vectors...\")\n",
        "model_avg = build_dense_model(embedding_dim)\n",
        "# model_avg.fit(X_avg_train_np, y_train_np, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model_avg.fit(\n",
        "    X_avg_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on average vectors...\")\n",
        "loss, accuracy_avg = model_avg.evaluate(X_avg_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Average Vector): {accuracy_avg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DKOmLByy_Mh",
        "outputId": "018733ff-9dc6-4cac-e520-810524de7159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model on max-pooled vectors...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4841 - loss: 0.7676 - val_accuracy: 0.5875 - val_loss: 0.6774\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5589 - loss: 0.6832 - val_accuracy: 0.6187 - val_loss: 0.6645\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5839 - loss: 0.6666 - val_accuracy: 0.6625 - val_loss: 0.6383\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6489 - loss: 0.6395 - val_accuracy: 0.7063 - val_loss: 0.6090\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 0.5903 - val_accuracy: 0.6750 - val_loss: 0.5865\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7297 - loss: 0.5411 - val_accuracy: 0.6938 - val_loss: 0.5641\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7668 - loss: 0.5071 - val_accuracy: 0.7125 - val_loss: 0.5498\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7977 - loss: 0.4509 - val_accuracy: 0.7188 - val_loss: 0.5499\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8123 - loss: 0.4291 - val_accuracy: 0.7000 - val_loss: 0.5537\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8247 - loss: 0.3875 - val_accuracy: 0.7312 - val_loss: 0.5733\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8507 - loss: 0.3357 - val_accuracy: 0.7125 - val_loss: 0.6081\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8592 - loss: 0.3142 - val_accuracy: 0.6875 - val_loss: 0.6198\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Evaluating model on max-pooled vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7310 - loss: 0.5801  \n",
            "Test Accuracy (Max Pooling Vector): 0.7050\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  X_max_train_np_scaled = scaler.fit_transform(X_max_train_np)\n",
        "  X_max_test_np_scaled = scaler.transform(X_max_test_np)\n",
        "else:\n",
        "  X_max_train_np_scaled = X_max_train_np\n",
        "  X_max_test_np_scaled = X_max_test_np\n",
        "\n",
        "\n",
        "\n",
        "# Train on max-pooled vectors\n",
        "print(\"Training model on max-pooled vectors...\")\n",
        "model_max = build_dense_model(embedding_dim)\n",
        "model_max.fit(\n",
        "    X_max_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on max-pooled vectors...\")\n",
        "loss, accuracy_max = model_max.evaluate(X_max_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Max Pooling Vector): {accuracy_max:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VkCwoh2erS7m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "def build_sequence_model(seq_len: int, embedding_dim: int) -> tf.keras.Model:\n",
        "    inputs = Input(shape=(seq_len, embedding_dim))\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NL22DOrXsljc"
      },
      "outputs": [],
      "source": [
        "def review_to_sequences(words: List[str], seq_len: int = 10) -> List[np.ndarray]:\n",
        "    \"\"\"Convert a list of words into non-overlapping sequences of word vectors.\"\"\"\n",
        "    vectors = [w2v_model.wv[w.lower()] for w in words if w.lower() in w2v_model.wv]\n",
        "    sequences = []\n",
        "    for i in range(0, len(vectors) - seq_len + 1, seq_len):\n",
        "        seq = vectors[i:i + seq_len]\n",
        "        sequences.append(np.stack(seq))  # shape: (seq_len, embedding_dim)\n",
        "    return sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "54nXfy5XsnMh"
      },
      "outputs": [],
      "source": [
        "sequence_length = 15\n",
        "\n",
        "\n",
        "# Shuffle and split into train/test by review (not sequences)\n",
        "train_reviews, test_reviews = train_test_split(documents, test_size=0.2, random_state=42)\n",
        "# Sequence training data from train_reviews\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for words, label in train_reviews:\n",
        "    sequences = review_to_sequences(words, seq_len=sequence_length)\n",
        "    X_seq.extend(sequences)\n",
        "    y_seq.extend([1 if label == 'pos' else 0] * len(sequences))\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iTC0zqCs5uV",
        "outputId": "20331877-9742-456d-de30-4cea01e5893e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5481 - loss: 0.6838 - val_accuracy: 0.6466 - val_loss: 0.6190\n",
            "Epoch 2/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.6596 - loss: 0.6187 - val_accuracy: 0.6666 - val_loss: 0.6068\n",
            "Epoch 3/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.6785 - loss: 0.5987 - val_accuracy: 0.6701 - val_loss: 0.6027\n",
            "Epoch 4/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6931 - loss: 0.5819 - val_accuracy: 0.6812 - val_loss: 0.5955\n",
            "Epoch 5/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7070 - loss: 0.5677 - val_accuracy: 0.6767 - val_loss: 0.5983\n",
            "Epoch 6/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.7182 - loss: 0.5530 - val_accuracy: 0.6647 - val_loss: 0.6031\n",
            "Epoch 7/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.7333 - loss: 0.5381 - val_accuracy: 0.6684 - val_loss: 0.6000\n",
            "Epoch 8/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7351 - loss: 0.5296 - val_accuracy: 0.6671 - val_loss: 0.6109\n",
            "Epoch 9/30\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7476 - loss: 0.5168 - val_accuracy: 0.6686 - val_loss: 0.6089\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6743 - loss: 0.5983\n",
            "Sequence Model Accuracy: 0.6772\n"
          ]
        }
      ],
      "source": [
        "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "model_seq = build_sequence_model(sequence_length, embedding_dim)\n",
        "model_seq.fit(\n",
        "    X_seq_train, y_seq_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "loss, acc = model_seq.evaluate(X_seq_test, y_seq_test)\n",
        "print(f\"Sequence Model Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QD47Feb1tULS"
      },
      "outputs": [],
      "source": [
        "def predict_review(words: List[str], model, seq_len: int = 10) -> float:\n",
        "    \"\"\"Predict review sentiment by averaging predictions over all sequences.\"\"\"\n",
        "    sequences = review_to_sequences(words, seq_len)\n",
        "    if not sequences:\n",
        "        return 0.5  # neutral prediction\n",
        "    sequences_np = np.array(sequences)\n",
        "    preds = model.predict(sequences_np, verbose=0)\n",
        "    return float(np.mean(preds))  # average prediction across sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ch12ny0tVQ5",
        "outputId": "5b0d1a44-61cc-4c41-8c64-75a59cea5662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sentiment score: 0.658 → Positive\n"
          ]
        }
      ],
      "source": [
        "sample_review_words = movie_reviews.words(movie_reviews.fileids('pos')[0])\n",
        "score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "print(f\"Predicted sentiment score: {score:.3f} → {'Positive' if score >= 0.5 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sX8CakMRev",
        "outputId": "57bce9dc-2b1d-4775-f545-f0df4147a9ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review-level accuracy (Case C): 0.8825\n"
          ]
        }
      ],
      "source": [
        "# Run review-level evaluation\n",
        "correct = 0\n",
        "for words, label in test_reviews:\n",
        "    score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "    pred_label = 'pos' if score >= 0.5 else 'neg'\n",
        "    if pred_label == label:\n",
        "        correct += 1\n",
        "\n",
        "review_accuracy = correct / len(test_reviews)\n",
        "print(f\"Review-level accuracy (Case C): {review_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
