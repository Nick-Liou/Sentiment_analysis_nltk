{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KN_sBufai4c_",
        "outputId": "d7060543-a66a-4f6d-e60c-3ae55bb2105b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gensim\n",
        "# Made with help from GPT\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from typing import List, Optional, Any\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "# nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ocwqm-ayjDFL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and shuffle documents\n",
        "documents : list[tuple[list[str], str]]  = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Extract only the review texts for training Word2Vec\n",
        "sentences = [list(map(str.lower, movie_reviews.words(fileid)))\n",
        "             for fileid in movie_reviews.fileids()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GnXjFN_jFKq"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model (Skip-Gram)\n",
        "embedding_dim = 300\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=embedding_dim,\n",
        "    window=15,\n",
        "    min_count=2,\n",
        "    sg=1,  # Skip-Gram\n",
        "    workers=8,\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lfjWwzD5jKkb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_vector(word: str) -> Optional[np.ndarray]:\n",
        "    \"\"\"Fetch vector from Word2Vec model if word exists.\"\"\"\n",
        "    return w2v_model.wv[word] if word in w2v_model.wv else None\n",
        "\n",
        "def get_review_vector_avg(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute average vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "def get_review_vector_max(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute max-pooled vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.max(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "X_avg = []\n",
        "X_max = []\n",
        "y = []\n",
        "\n",
        "for words, label in documents:\n",
        "    X_avg.append(get_review_vector_avg(words))\n",
        "    X_max.append(get_review_vector_max(words))\n",
        "    y.append(1 if label == 'pos' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MxHyPMCjLNu",
        "outputId": "929b43e9-8004-4207-b6c8-0bb97b00ffda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87       212\n",
            "           1       0.85      0.86      0.85       188\n",
            "\n",
            "    accuracy                           0.86       400\n",
            "   macro avg       0.86      0.86      0.86       400\n",
            "weighted avg       0.86      0.86      0.86       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.70      0.72       212\n",
            "           1       0.68      0.74      0.71       188\n",
            "\n",
            "    accuracy                           0.72       400\n",
            "   macro avg       0.72      0.72      0.72       400\n",
            "weighted avg       0.72      0.72      0.72       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg = LogisticRegression(max_iter=1000)\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max = LogisticRegression(max_iter=1000)\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMwc96EF3n-3",
        "outputId": "a91e623c-0b96-4e51-85a2-04ccbbe0daeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.91       212\n",
            "           1       0.89      0.92      0.90       188\n",
            "\n",
            "    accuracy                           0.91       400\n",
            "   macro avg       0.91      0.91      0.91       400\n",
            "weighted avg       0.91      0.91      0.91       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.71      0.71       212\n",
            "           1       0.68      0.69      0.68       188\n",
            "\n",
            "    accuracy                           0.70       400\n",
            "   macro avg       0.70      0.70      0.70       400\n",
            "weighted avg       0.70      0.70      0.70       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "scale_data = True\n",
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "if scale_data:\n",
        "    clf_avg = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "    clf_max = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "else:\n",
        "    clf_avg = LogisticRegression(max_iter=1000)\n",
        "    clf_max = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L9VjDTCljNwf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def build_dense_model(input_dim: int) -> tf.keras.Model:\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),             # Cleanly define input shape\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')         # Binary classification\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERVuUaLK0Alj",
        "outputId": "8a3ac85e-9f03-4a33-b963-8ea4c8254e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train labels: (array([0, 1]), array([788, 812]))\n",
            "Test labels: (array([0, 1]), array([212, 188]))\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "X_avg_train_np = np.array(X_avg_train)\n",
        "X_avg_test_np = np.array(X_avg_test)\n",
        "X_max_train_np = np.array(X_max_train)\n",
        "X_max_test_np = np.array(X_max_test)\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(\"Train labels:\", np.unique(y_train_np, return_counts=True))\n",
        "print(\"Test labels:\", np.unique(y_test_np, return_counts=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRHgdB9jTIF",
        "outputId": "20e6a1a5-433d-49c8-d690-3bb8ca8240de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Scaled!\n",
            "Training model on average vectors...\n",
            "Epoch 1/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.5536 - loss: 0.7516 - val_accuracy: 0.7563 - val_loss: 0.5042\n",
            "Epoch 2/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7460 - loss: 0.5209 - val_accuracy: 0.8313 - val_loss: 0.3990\n",
            "Epoch 3/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7910 - loss: 0.4423 - val_accuracy: 0.8625 - val_loss: 0.3316\n",
            "Epoch 4/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8857 - loss: 0.2999 - val_accuracy: 0.8813 - val_loss: 0.3086\n",
            "Epoch 5/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8993 - loss: 0.2676 - val_accuracy: 0.8813 - val_loss: 0.2833\n",
            "Epoch 6/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9241 - loss: 0.2004 - val_accuracy: 0.8875 - val_loss: 0.2517\n",
            "Epoch 7/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9398 - loss: 0.1640 - val_accuracy: 0.8813 - val_loss: 0.2801\n",
            "Epoch 8/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9508 - loss: 0.1508 - val_accuracy: 0.8813 - val_loss: 0.2772\n",
            "Epoch 9/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9556 - loss: 0.1232 - val_accuracy: 0.8687 - val_loss: 0.3049\n",
            "Epoch 10/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9589 - loss: 0.1075 - val_accuracy: 0.8875 - val_loss: 0.3230\n",
            "Epoch 11/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.0904 - val_accuracy: 0.9062 - val_loss: 0.3135\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Evaluating model on average vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8880 - loss: 0.2655  \n",
            "Test Accuracy (Average Vector): 0.8950\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_avg_train_np_scaled = scaler.fit_transform(X_avg_train_np)\n",
        "  X_avg_test_np_scaled = scaler.transform(X_avg_test_np)\n",
        "  print(\"Data Scaled!\")\n",
        "else:\n",
        "  X_avg_train_np_scaled = X_avg_train_np\n",
        "  X_avg_test_np_scaled = X_avg_test_np\n",
        "\n",
        "\n",
        "# Train on average vectors\n",
        "print(\"Training model on average vectors...\")\n",
        "model_avg = build_dense_model(embedding_dim)\n",
        "# model_avg.fit(X_avg_train_np, y_train_np, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model_avg.fit(\n",
        "    X_avg_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on average vectors...\")\n",
        "loss, accuracy_avg = model_avg.evaluate(X_avg_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Average Vector): {accuracy_avg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DKOmLByy_Mh",
        "outputId": "7597c57d-e481-44fb-c6df-430fb4cd9ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model on max-pooled vectors...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5208 - loss: 0.7838 - val_accuracy: 0.5500 - val_loss: 0.7007\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5751 - loss: 0.6815 - val_accuracy: 0.5625 - val_loss: 0.6896\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6048 - loss: 0.6490 - val_accuracy: 0.6187 - val_loss: 0.6671\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6723 - loss: 0.6131 - val_accuracy: 0.6375 - val_loss: 0.6370\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7475 - loss: 0.5365 - val_accuracy: 0.6687 - val_loss: 0.6207\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7625 - loss: 0.4896 - val_accuracy: 0.6562 - val_loss: 0.6352\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7989 - loss: 0.4551 - val_accuracy: 0.6687 - val_loss: 0.6253\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8447 - loss: 0.3691 - val_accuracy: 0.7125 - val_loss: 0.6373\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8275 - loss: 0.3503 - val_accuracy: 0.6750 - val_loss: 0.6907\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8789 - loss: 0.3038 - val_accuracy: 0.7312 - val_loss: 0.6853\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Evaluating model on max-pooled vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6418 - loss: 0.6187  \n",
            "Test Accuracy (Max Pooling Vector): 0.6925\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  X_max_train_np_scaled = scaler.fit_transform(X_max_train_np)\n",
        "  X_max_test_np_scaled = scaler.transform(X_max_test_np)\n",
        "else:\n",
        "  X_max_train_np_scaled = X_max_train_np\n",
        "  X_max_test_np_scaled = X_max_test_np\n",
        "\n",
        "\n",
        "\n",
        "# Train on max-pooled vectors\n",
        "print(\"Training model on max-pooled vectors...\")\n",
        "model_max = build_dense_model(embedding_dim)\n",
        "model_max.fit(\n",
        "    X_max_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on max-pooled vectors...\")\n",
        "loss, accuracy_max = model_max.evaluate(X_max_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Max Pooling Vector): {accuracy_max:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VkCwoh2erS7m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "def build_sequence_model(seq_len: int, embedding_dim: int) -> tf.keras.Model:\n",
        "    inputs = Input(shape=(seq_len, embedding_dim))\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NL22DOrXsljc"
      },
      "outputs": [],
      "source": [
        "def review_to_sequences(words: List[str], seq_len: int = 10) -> List[np.ndarray]:\n",
        "    \"\"\"Convert a list of words into non-overlapping sequences of word vectors.\"\"\"\n",
        "    vectors = [w2v_model.wv[w.lower()] for w in words if w.lower() in w2v_model.wv]\n",
        "    sequences = []\n",
        "    for i in range(0, len(vectors) - seq_len + 1, seq_len):\n",
        "        seq = vectors[i:i + seq_len]\n",
        "        sequences.append(np.stack(seq))  # shape: (seq_len, embedding_dim)\n",
        "    return sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "54nXfy5XsnMh"
      },
      "outputs": [],
      "source": [
        "sequence_length = 15\n",
        "\n",
        "\n",
        "# Shuffle and split into train/test by review (not sequences)\n",
        "train_reviews, test_reviews = train_test_split(documents, test_size=0.2, random_state=42)\n",
        "# Sequence training data from train_reviews\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for words, label in train_reviews:\n",
        "    sequences = review_to_sequences(words, seq_len=sequence_length)\n",
        "    X_seq.extend(sequences)\n",
        "    y_seq.extend([1 if label == 'pos' else 0] * len(sequences))\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iTC0zqCs5uV",
        "outputId": "0b61d4a7-3a23-4a2c-9412-75750d3b0c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.5537 - loss: 0.6810 - val_accuracy: 0.6500 - val_loss: 0.6195\n",
            "Epoch 2/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.6604 - loss: 0.6171 - val_accuracy: 0.6705 - val_loss: 0.6037\n",
            "Epoch 3/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.6797 - loss: 0.5923 - val_accuracy: 0.6657 - val_loss: 0.6014\n",
            "Epoch 4/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.7031 - loss: 0.5738 - val_accuracy: 0.6687 - val_loss: 0.5981\n",
            "Epoch 5/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.7113 - loss: 0.5601 - val_accuracy: 0.6671 - val_loss: 0.5979\n",
            "Epoch 6/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.7271 - loss: 0.5426 - val_accuracy: 0.6610 - val_loss: 0.6061\n",
            "Epoch 7/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.7359 - loss: 0.5307 - val_accuracy: 0.6665 - val_loss: 0.6113\n",
            "Epoch 8/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.7439 - loss: 0.5204 - val_accuracy: 0.6615 - val_loss: 0.6113\n",
            "Epoch 9/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.7560 - loss: 0.5026 - val_accuracy: 0.6663 - val_loss: 0.6125\n",
            "Epoch 10/30\n",
            "\u001b[1m933/933\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.7616 - loss: 0.4927 - val_accuracy: 0.6636 - val_loss: 0.6185\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6774 - loss: 0.5959\n",
            "Sequence Model Accuracy: 0.6786\n"
          ]
        }
      ],
      "source": [
        "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "model_seq = build_sequence_model(sequence_length, embedding_dim)\n",
        "model_seq.fit(\n",
        "    X_seq_train, y_seq_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "loss, acc = model_seq.evaluate(X_seq_test, y_seq_test)\n",
        "print(f\"Sequence Model Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QD47Feb1tULS"
      },
      "outputs": [],
      "source": [
        "def predict_review(words: List[str], model, seq_len: int = 10) -> float:\n",
        "    \"\"\"Predict review sentiment by averaging predictions over all sequences.\"\"\"\n",
        "    sequences = review_to_sequences(words, seq_len)\n",
        "    if not sequences:\n",
        "        return 0.5  # neutral prediction\n",
        "    sequences_np = np.array(sequences)\n",
        "    preds = model.predict(sequences_np, verbose=0)\n",
        "    return float(np.mean(preds))  # average prediction across sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ch12ny0tVQ5",
        "outputId": "6e3c54d5-d971-4f46-d2ad-9f866e67199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sentiment score: 0.680 → Positive\n"
          ]
        }
      ],
      "source": [
        "sample_review_words = movie_reviews.words(movie_reviews.fileids('pos')[0])\n",
        "score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "print(f\"Predicted sentiment score: {score:.3f} → {'Positive' if score >= 0.5 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sX8CakMRev",
        "outputId": "f6e49bd8-f1e4-44a4-ef95-5c292907236d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review-level accuracy (Case C): 0.9050\n"
          ]
        }
      ],
      "source": [
        "# Run review-level evaluation\n",
        "correct = 0\n",
        "for words, label in test_reviews:\n",
        "    score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "    pred_label = 'pos' if score >= 0.5 else 'neg'\n",
        "    if pred_label == label:\n",
        "        correct += 1\n",
        "\n",
        "review_accuracy = correct / len(test_reviews)\n",
        "print(f\"Review-level accuracy (Case C): {review_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
