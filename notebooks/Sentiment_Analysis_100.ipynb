{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KN_sBufai4c_",
        "outputId": "31783c5c-6b99-4a00-f8ad-b976115ade65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gensim\n",
        "# Made with help from GPT\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from typing import List, Optional, Any\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "# nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ocwqm-ayjDFL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and shuffle documents\n",
        "documents : list[tuple[list[str], str]]  = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Extract only the review texts for training Word2Vec\n",
        "sentences = [list(map(str.lower, movie_reviews.words(fileid)))\n",
        "             for fileid in movie_reviews.fileids()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GnXjFN_jFKq"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model (Skip-Gram)\n",
        "embedding_dim = 100\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=embedding_dim,\n",
        "    window=15,\n",
        "    min_count=2,\n",
        "    sg=1,  # Skip-Gram\n",
        "    workers=8,\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lfjWwzD5jKkb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_vector(word: str) -> Optional[np.ndarray]:\n",
        "    \"\"\"Fetch vector from Word2Vec model if word exists.\"\"\"\n",
        "    return w2v_model.wv[word] if word in w2v_model.wv else None\n",
        "\n",
        "def get_review_vector_avg(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute average vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "def get_review_vector_max(words: List[str]) -> np.ndarray:\n",
        "    \"\"\"Compute max-pooled vector for a list of words.\"\"\"\n",
        "    vectors = [w2v_model.wv[w] for w in map(str.lower, words) if w in w2v_model.wv]\n",
        "    return np.max(vectors, axis=0) if vectors else np.zeros(embedding_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "X_avg = []\n",
        "X_max = []\n",
        "y = []\n",
        "\n",
        "for words, label in documents:\n",
        "    X_avg.append(get_review_vector_avg(words))\n",
        "    X_max.append(get_review_vector_max(words))\n",
        "    y.append(1 if label == 'pos' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MxHyPMCjLNu",
        "outputId": "37c65cc4-057a-48c4-e5f7-886453365140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.83      0.77       184\n",
            "           1       0.83      0.72      0.77       216\n",
            "\n",
            "    accuracy                           0.77       400\n",
            "   macro avg       0.77      0.77      0.77       400\n",
            "weighted avg       0.78      0.77      0.77       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.64      0.61       184\n",
            "           1       0.67      0.62      0.64       216\n",
            "\n",
            "    accuracy                           0.63       400\n",
            "   macro avg       0.63      0.63      0.63       400\n",
            "weighted avg       0.63      0.63      0.63       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg = LogisticRegression(max_iter=1000)\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max = LogisticRegression(max_iter=1000)\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMwc96EF3n-3",
        "outputId": "76b53e83-760c-4451-e487-9a9be6c1abe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report (Average Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87       184\n",
            "           1       0.89      0.88      0.89       216\n",
            "\n",
            "    accuracy                           0.88       400\n",
            "   macro avg       0.88      0.88      0.88       400\n",
            "weighted avg       0.88      0.88      0.88       400\n",
            "\n",
            "Classification Report (Max Pooling Vector):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59       184\n",
            "           1       0.65      0.61      0.63       216\n",
            "\n",
            "    accuracy                           0.61       400\n",
            "   macro avg       0.61      0.61      0.61       400\n",
            "weighted avg       0.61      0.61      0.61       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "scale_data = True\n",
        "\n",
        "# Split data\n",
        "X_avg_train, X_avg_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)\n",
        "X_max_train, X_max_test, _, _ = train_test_split(X_max, y, test_size=0.2, random_state=42)\n",
        "\n",
        "if scale_data:\n",
        "    clf_avg = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "    clf_max = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "else:\n",
        "    clf_avg = LogisticRegression(max_iter=1000)\n",
        "    clf_max = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train classifiers\n",
        "clf_avg.fit(X_avg_train, y_train)\n",
        "y_pred_avg = clf_avg.predict(X_avg_test)\n",
        "\n",
        "clf_max.fit(X_max_train, y_train)\n",
        "y_pred_max = clf_max.predict(X_max_test)\n",
        "\n",
        "# Results\n",
        "print(\"Classification Report (Average Vector):\")\n",
        "print(classification_report(y_test, y_pred_avg))\n",
        "\n",
        "print(\"Classification Report (Max Pooling Vector):\")\n",
        "print(classification_report(y_test, y_pred_max))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "L9VjDTCljNwf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def build_dense_model(input_dim: int) -> tf.keras.Model:\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),             # Cleanly define input shape\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')         # Binary classification\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERVuUaLK0Alj",
        "outputId": "4b995e99-32b4-4134-c437-aaf675769a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train labels: (array([0, 1]), array([816, 784]))\n",
            "Test labels: (array([0, 1]), array([184, 216]))\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "X_avg_train_np = np.array(X_avg_train)\n",
        "X_avg_test_np = np.array(X_avg_test)\n",
        "X_max_train_np = np.array(X_max_train)\n",
        "X_max_test_np = np.array(X_max_test)\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(\"Train labels:\", np.unique(y_train_np, return_counts=True))\n",
        "print(\"Test labels:\", np.unique(y_test_np, return_counts=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRHgdB9jTIF",
        "outputId": "82a3bd5d-345e-448f-f8b5-023034d5d864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Scaled!\n",
            "Training model on average vectors...\n",
            "Epoch 1/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5694 - loss: 0.7738 - val_accuracy: 0.7688 - val_loss: 0.5077\n",
            "Epoch 2/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6649 - loss: 0.6037 - val_accuracy: 0.8188 - val_loss: 0.4255\n",
            "Epoch 3/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7822 - loss: 0.4628 - val_accuracy: 0.8625 - val_loss: 0.3255\n",
            "Epoch 4/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8342 - loss: 0.3748 - val_accuracy: 0.8687 - val_loss: 0.2702\n",
            "Epoch 5/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8544 - loss: 0.3399 - val_accuracy: 0.9000 - val_loss: 0.2246\n",
            "Epoch 6/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8788 - loss: 0.2704 - val_accuracy: 0.9062 - val_loss: 0.2152\n",
            "Epoch 7/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9006 - loss: 0.2468 - val_accuracy: 0.9375 - val_loss: 0.2011\n",
            "Epoch 8/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9136 - loss: 0.2141 - val_accuracy: 0.9438 - val_loss: 0.1509\n",
            "Epoch 9/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9139 - loss: 0.2155 - val_accuracy: 0.9438 - val_loss: 0.1755\n",
            "Epoch 10/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9264 - loss: 0.1890 - val_accuracy: 0.9500 - val_loss: 0.1428\n",
            "Epoch 11/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9453 - loss: 0.1499 - val_accuracy: 0.9563 - val_loss: 0.1414\n",
            "Epoch 12/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9449 - loss: 0.1555 - val_accuracy: 0.9563 - val_loss: 0.1558\n",
            "Epoch 13/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9512 - loss: 0.1370 - val_accuracy: 0.9500 - val_loss: 0.1629\n",
            "Epoch 14/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9450 - loss: 0.1434 - val_accuracy: 0.9563 - val_loss: 0.1587\n",
            "Epoch 15/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9681 - loss: 0.0967 - val_accuracy: 0.9500 - val_loss: 0.1626\n",
            "Epoch 16/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9554 - loss: 0.1124 - val_accuracy: 0.9438 - val_loss: 0.1403\n",
            "Epoch 17/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9627 - loss: 0.1152 - val_accuracy: 0.9563 - val_loss: 0.1682\n",
            "Epoch 18/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9662 - loss: 0.0897 - val_accuracy: 0.9625 - val_loss: 0.1300\n",
            "Epoch 19/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.0918 - val_accuracy: 0.9500 - val_loss: 0.1643\n",
            "Epoch 20/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0821 - val_accuracy: 0.9563 - val_loss: 0.1318\n",
            "Epoch 21/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0658 - val_accuracy: 0.9625 - val_loss: 0.1307\n",
            "Epoch 22/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9716 - loss: 0.0688 - val_accuracy: 0.9500 - val_loss: 0.1641\n",
            "Epoch 23/50\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0636 - val_accuracy: 0.9438 - val_loss: 0.1898\n",
            "Epoch 23: early stopping\n",
            "Restoring model weights from the end of the best epoch: 18.\n",
            "Evaluating model on average vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9101 - loss: 0.3064  \n",
            "Test Accuracy (Average Vector): 0.8900\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_avg_train_np_scaled = scaler.fit_transform(X_avg_train_np)\n",
        "  X_avg_test_np_scaled = scaler.transform(X_avg_test_np)\n",
        "  print(\"Data Scaled!\")\n",
        "else:\n",
        "  X_avg_train_np_scaled = X_avg_train_np\n",
        "  X_avg_test_np_scaled = X_avg_test_np\n",
        "\n",
        "\n",
        "# Train on average vectors\n",
        "print(\"Training model on average vectors...\")\n",
        "model_avg = build_dense_model(embedding_dim)\n",
        "# model_avg.fit(X_avg_train_np, y_train_np, epochs=10, batch_size=32, validation_split=0.1)\n",
        "model_avg.fit(\n",
        "    X_avg_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on average vectors...\")\n",
        "loss, accuracy_avg = model_avg.evaluate(X_avg_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Average Vector): {accuracy_avg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DKOmLByy_Mh",
        "outputId": "70de9cb2-ffac-4a6e-ec3c-fe3edd8273bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model on max-pooled vectors...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5263 - loss: 0.7352 - val_accuracy: 0.5063 - val_loss: 0.6868\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5718 - loss: 0.6745 - val_accuracy: 0.5813 - val_loss: 0.6716\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5651 - loss: 0.6692 - val_accuracy: 0.6187 - val_loss: 0.6616\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6419 - loss: 0.6378 - val_accuracy: 0.6187 - val_loss: 0.6501\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6570 - loss: 0.6223 - val_accuracy: 0.6438 - val_loss: 0.6429\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6919 - loss: 0.5868 - val_accuracy: 0.6125 - val_loss: 0.6436\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7239 - loss: 0.5662 - val_accuracy: 0.6500 - val_loss: 0.6358\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7184 - loss: 0.5530 - val_accuracy: 0.6187 - val_loss: 0.6486\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7501 - loss: 0.5112 - val_accuracy: 0.6375 - val_loss: 0.6526\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7455 - loss: 0.5131 - val_accuracy: 0.6250 - val_loss: 0.6544\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7649 - loss: 0.4820 - val_accuracy: 0.6375 - val_loss: 0.6649\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7930 - loss: 0.4632 - val_accuracy: 0.6375 - val_loss: 0.6815\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Evaluating model on max-pooled vectors...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6538 - loss: 0.6543  \n",
            "Test Accuracy (Max Pooling Vector): 0.6425\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if scale_data:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  X_max_train_np_scaled = scaler.fit_transform(X_max_train_np)\n",
        "  X_max_test_np_scaled = scaler.transform(X_max_test_np)\n",
        "else:\n",
        "  X_max_train_np_scaled = X_max_train_np\n",
        "  X_max_test_np_scaled = X_max_test_np\n",
        "\n",
        "\n",
        "\n",
        "# Train on max-pooled vectors\n",
        "print(\"Training model on max-pooled vectors...\")\n",
        "model_max = build_dense_model(embedding_dim)\n",
        "model_max.fit(\n",
        "    X_max_train_np_scaled,\n",
        "    y_train_np,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating model on max-pooled vectors...\")\n",
        "loss, accuracy_max = model_max.evaluate(X_max_test_np_scaled, y_test_np)\n",
        "print(f\"Test Accuracy (Max Pooling Vector): {accuracy_max:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "VkCwoh2erS7m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "def build_sequence_model(seq_len: int, embedding_dim: int) -> tf.keras.Model:\n",
        "    inputs = Input(shape=(seq_len, embedding_dim))\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NL22DOrXsljc"
      },
      "outputs": [],
      "source": [
        "def review_to_sequences(words: List[str], seq_len: int = 10) -> List[np.ndarray]:\n",
        "    \"\"\"Convert a list of words into non-overlapping sequences of word vectors.\"\"\"\n",
        "    vectors = [w2v_model.wv[w.lower()] for w in words if w.lower() in w2v_model.wv]\n",
        "    sequences = []\n",
        "    for i in range(0, len(vectors) - seq_len + 1, seq_len):\n",
        "        seq = vectors[i:i + seq_len]\n",
        "        sequences.append(np.stack(seq))  # shape: (seq_len, embedding_dim)\n",
        "    return sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "54nXfy5XsnMh"
      },
      "outputs": [],
      "source": [
        "sequence_length = 15\n",
        "\n",
        "\n",
        "# Shuffle and split into train/test by review (not sequences)\n",
        "train_reviews, test_reviews = train_test_split(documents, test_size=0.2, random_state=42)\n",
        "# Sequence training data from train_reviews\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for words, label in train_reviews:\n",
        "    sequences = review_to_sequences(words, seq_len=sequence_length)\n",
        "    X_seq.extend(sequences)\n",
        "    y_seq.extend([1 if label == 'pos' else 0] * len(sequences))\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iTC0zqCs5uV",
        "outputId": "36de3ea8-7b68-489c-b223-96a5197d7ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.5469 - loss: 0.6818 - val_accuracy: 0.6490 - val_loss: 0.6243\n",
            "Epoch 2/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.6602 - loss: 0.6178 - val_accuracy: 0.6672 - val_loss: 0.6046\n",
            "Epoch 3/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.6854 - loss: 0.5987 - val_accuracy: 0.6718 - val_loss: 0.6053\n",
            "Epoch 4/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.6918 - loss: 0.5894 - val_accuracy: 0.6787 - val_loss: 0.5952\n",
            "Epoch 5/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7003 - loss: 0.5785 - val_accuracy: 0.6797 - val_loss: 0.5933\n",
            "Epoch 6/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7075 - loss: 0.5700 - val_accuracy: 0.6776 - val_loss: 0.5944\n",
            "Epoch 7/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7089 - loss: 0.5676 - val_accuracy: 0.6739 - val_loss: 0.5976\n",
            "Epoch 8/30\n",
            "\u001b[1m943/943\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7146 - loss: 0.5612 - val_accuracy: 0.6794 - val_loss: 0.6011\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6800 - loss: 0.5921\n",
            "Sequence Model Accuracy: 0.6792\n"
          ]
        }
      ],
      "source": [
        "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "model_seq = build_sequence_model(sequence_length, embedding_dim)\n",
        "model_seq.fit(\n",
        "    X_seq_train, y_seq_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "loss, acc = model_seq.evaluate(X_seq_test, y_seq_test)\n",
        "print(f\"Sequence Model Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QD47Feb1tULS"
      },
      "outputs": [],
      "source": [
        "def predict_review(words: List[str], model, seq_len: int = 10) -> float:\n",
        "    \"\"\"Predict review sentiment by averaging predictions over all sequences.\"\"\"\n",
        "    sequences = review_to_sequences(words, seq_len)\n",
        "    if not sequences:\n",
        "        return 0.5  # neutral prediction\n",
        "    sequences_np = np.array(sequences)\n",
        "    preds = model.predict(sequences_np, verbose=0)\n",
        "    return float(np.mean(preds))  # average prediction across sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ch12ny0tVQ5",
        "outputId": "e472d866-b2fa-4113-ae8c-838fc58866dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sentiment score: 0.673 → Positive\n"
          ]
        }
      ],
      "source": [
        "sample_review_words = movie_reviews.words(movie_reviews.fileids('pos')[0])\n",
        "score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "print(f\"Predicted sentiment score: {score:.3f} → {'Positive' if score >= 0.5 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sX8CakMRev",
        "outputId": "6ad13643-dacd-4970-f7dc-62d1988cc8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review-level accuracy (Case C): 0.8675\n"
          ]
        }
      ],
      "source": [
        "# Run review-level evaluation\n",
        "correct = 0\n",
        "for words, label in test_reviews:\n",
        "    score = predict_review(words, model_seq, seq_len=sequence_length)\n",
        "    pred_label = 'pos' if score >= 0.5 else 'neg'\n",
        "    if pred_label == label:\n",
        "        correct += 1\n",
        "\n",
        "review_accuracy = correct / len(test_reviews)\n",
        "print(f\"Review-level accuracy (Case C): {review_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
